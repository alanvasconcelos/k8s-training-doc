[
{
	"uri": "/labels_and_annotations/01-labels/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Why we need labels ? If you have a bucket of white dominos and you want to group it based on the number of dots.\nLets say we want all dominos with 10 dots; we will take domino one by one and if its having 10 dots ,we will put it aside and continue the same operation until all dominos were checked.\nLikewise , suppose if you have 100 pods and few of them are nginx and few of them are centos , how we can see only nginx pods ?\nWe need a label on each pod so that we can tell kubectl command to show the pods with that label.\nIn kubernetes , label is a key value pair and it provides \u0026lsquo;identifying metadata\u0026rsquo; for objects. These are fundamental qualities of objects that will be used for grouping , viewing and operating.\nFor now we will se how we can view them (Will discuss about grouping and operation on pod groups later)\nPod labels Lets run a Coffee app Pod\nk8s@k8s-master-01:~$ kubectl run coffee-app --image=ansilh/demo-coffee --restart=Never pod/coffee-app created k8s@k8s-master-01:~$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 4s k8s@k8s-master-01:~$  See the labels of a Pods k8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 37s run=coffee-app k8s@k8s-master-01:~$  As you can see above , the lables is run=coffee-app which is a key value pair - key is run value is coffee-app. When we run Pod imperatively , kubectl ass this label to Pod.\nAdd custom label to Pod We can add label to Pod using kubectl label command.\nk8s@k8s-master-01:~$ kubectl label pod coffee-app app=frontend pod/coffee-app labeled k8s@k8s-master-01:~$  Here we have add a label app=frontend to pod coffee-app.\nUse label selectors Lets start another coffee application pod with name coffee-app02.\nk8s@k8s-master-01:~$ kubectl run coffee-app02 --image=ansilh/demo-coffee --restart=Never pod/coffee-app02 created k8s@k8s-master-01:~$  Now we have two Pods.\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 5m5s app=frontend,run=coffee-app coffee-app02 1/1 Running 0 20s run=coffee-app02 k8s@k8s-master-01:~$  Lets see how can I select the Pods with label app=frontend.\nk8s@k8s-master-01:~$ kubectl get pods --selector=app=frontend NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 6m52s k8s@k8s-master-01:~$  You can add as many as label you want.\nWe can add a prefix like app ( eg: app/dev=true ) which is also a valid label.\n   Limitations      Prefix DNS subdomain with 256 characters   Key 63 characters   Value 63 characters    Remove labels See the labels of coffee-app\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 28m app=frontend,run=coffee-app coffee-app02 1/1 Running 0 24m run=coffee-app02  Remove the app label\nk8s@k8s-master-01:~$ kubectl label pod coffee-app app- pod/coffee-app labeled  Resulting output\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 29m run=coffee-app coffee-app02 1/1 Running 0 24m run=coffee-app02 k8s@k8s-master-01:~$  "
},
{
	"uri": "/labels_and_annotations/02-annotations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Why we need annotations ? We can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.\nIts just a place to store more metadata which is not used for any selection , grouping or operations.\nAnnotate Pod Lets say , if you want to add a download URL to pod.\n$ kubectl annotate pod coffee-app url=https://hub.docker.com/r/ansilh/demo-webapp pod/coffee-app annotated  View annotations k8s@k8s-master-01:~$ kubectl describe pod coffee-app Name: coffee-app Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: k8s-worker-01/192.168.56.202 Start Time: Fri, 04 Jan 2019 00:47:10 +0530 Labels: app=frontend run=coffee-app Annotations: cni.projectcalico.org/podIP: 10.10.1.11/32 url: https://hub.docker.com/r/ansilh/demo-webapp Status: Running IP: 10.10.1.11 ...  Annotations filed containe two entries\ncni.projectcalico.org/podIP: 10.10.1.11/32\nurl: https://hub.docker.com/r/ansilh/demo-webapp\nRemove annotation Use same annotate command and mention only key with a dash (-) at the end of the key . Below command will remove the annotation url: https://hub.docker.com/r/ansilh/demo-webapp from Pod.\nk8s@k8s-master-01:~$ kubectl annotate pod coffee-app url- pod/coffee-app annotated k8s@k8s-master-01:~$  Annotation after removal.\nk8s@k8s-master-01:~$ kubectl describe pod coffee-app Name: coffee-app Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: k8s-worker-01/192.168.56.202 Start Time: Fri, 04 Jan 2019 00:47:10 +0530 Labels: app=frontend run=coffee-app Annotations: cni.projectcalico.org/podIP: 10.10.1.11/32 Status: Running IP: 10.10.1.11  "
},
{
	"uri": "/multi_container_pod/01-env-inject/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Inject data to pod via Environmental variable We will create a Coffee Pod\n$ kubectl run tea --image=ansilh/demo-tea --env=MY_NODE_NAME=scratch --restart=Never --dry-run -o yaml \u0026gt;pod-with-env.yaml  apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: tea name: tea spec: containers: - env: - name: MY_NODE_NAME value: scratch image: ansilh/demo-tea name: coffee-new resources: {} dnsPolicy: ClusterFirst restartPolicy: Never status: {}  Lets run this Pod\n$ kubectl create -f pod-with-env.yaml  $ kubectl get pods NAME READY STATUS RESTARTS AGE tea 1/1 Running 0 7s  Lets expose the pod as NodePort\n$ kubectl expose pod tea --port=80 --target-port=8080 --type=NodePort  $ kubectl get svc tea NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tea NodePort 192.168.10.37 \u0026lt;none\u0026gt; 80:32258/TCP 42s  Access the service using browser uisng node IP and port 32258\nYou will see below in Page Node:scratch\nExpose Pod fields to containers Lets extract the nodeName from spec ( Excuse me ? yeah we will see that in a moment )\nk8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.spec.nodeName}' \u0026amp;\u0026amp; echo k8s-worker-01 k8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.status.hostIP}' \u0026amp;\u0026amp; echo 192.168.56.202 k8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.status.podIP}' \u0026amp;\u0026amp; echo 10.10.1.23 k8s@k8s-master-01:~$  To get the JSON path , first we need to get the entire object output in JSON. We have used output in YAML so far because its easy . But internally kubectl convers YAML to JSON\n$ kubectl get pod tea -o json  { \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;annotations\u0026quot;: { \u0026quot;cni.projectcalico.org/podIP\u0026quot;: \u0026quot;10.10.1.23/32\u0026quot; }, \u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;labels\u0026quot;: { \u0026quot;run\u0026quot;: \u0026quot;tea\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;tea\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;resourceVersion\u0026quot;: \u0026quot;218696\u0026quot;, \u0026quot;selfLink\u0026quot;: \u0026quot;/api/v1/namespaces/default/pods/tea\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;14c1715b-11c5-11e9-9f0f-0800276a1bd2\u0026quot; }, \u0026quot;spec\u0026quot;: { \u0026quot;containers\u0026quot;: [ { \u0026quot;env\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;MY_NODE_NAME\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;scratch\u0026quot; } ], \u0026quot;image\u0026quot;: \u0026quot;ansilh/demo-tea\u0026quot;, \u0026quot;imagePullPolicy\u0026quot;: \u0026quot;Always\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;coffee-new\u0026quot;, \u0026quot;resources\u0026quot;: {}, \u0026quot;terminationMessagePath\u0026quot;: \u0026quot;/dev/termination-log\u0026quot;, \u0026quot;terminationMessagePolicy\u0026quot;: \u0026quot;File\u0026quot;, \u0026quot;volumeMounts\u0026quot;: [ { \u0026quot;mountPath\u0026quot;: \u0026quot;/var/run/secrets/kubernetes.io/serviceaccount\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;default-token-72pzg\u0026quot;, \u0026quot;readOnly\u0026quot;: true } ] } ], \u0026quot;dnsPolicy\u0026quot;: \u0026quot;ClusterFirst\u0026quot;, \u0026quot;enableServiceLinks\u0026quot;: true, \u0026quot;nodeName\u0026quot;: \u0026quot;k8s-worker-01\u0026quot;, \u0026quot;priority\u0026quot;: 0, \u0026quot;restartPolicy\u0026quot;: \u0026quot;Never\u0026quot;, \u0026quot;schedulerName\u0026quot;: \u0026quot;default-scheduler\u0026quot;, \u0026quot;securityContext\u0026quot;: {}, \u0026quot;serviceAccount\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;serviceAccountName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;terminationGracePeriodSeconds\u0026quot;: 30, \u0026quot;tolerations\u0026quot;: [ { \u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/not-ready\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;, \u0026quot;tolerationSeconds\u0026quot;: 300 }, { \u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/unreachable\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;, \u0026quot;tolerationSeconds\u0026quot;: 300 } ], \u0026quot;volumes\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default-token-72pzg\u0026quot;, \u0026quot;secret\u0026quot;: { \u0026quot;defaultMode\u0026quot;: 420, \u0026quot;secretName\u0026quot;: \u0026quot;default-token-72pzg\u0026quot; } } ] }, \u0026quot;status\u0026quot;: { \u0026quot;conditions\u0026quot;: [ { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Initialized\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Ready\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;ContainersReady\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;PodScheduled\u0026quot; } ], \u0026quot;containerStatuses\u0026quot;: [ { \u0026quot;containerID\u0026quot;: \u0026quot;docker://291a72e7fdab6a9f7afc47c640126cf596f5e071903b6a9055b44ef5bcb1c104\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;ansilh/demo-tea:latest\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;docker-pullable://ansilh/demo-tea@sha256:998d07a15151235132dae9781f587ea4d2822c62165778570145b0f659dda7bb\u0026quot;, \u0026quot;lastState\u0026quot;: {}, \u0026quot;name\u0026quot;: \u0026quot;coffee-new\u0026quot;, \u0026quot;ready\u0026quot;: true, \u0026quot;restartCount\u0026quot;: 0, \u0026quot;state\u0026quot;: { \u0026quot;running\u0026quot;: { \u0026quot;startedAt\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot; } } } ], \u0026quot;hostIP\u0026quot;: \u0026quot;192.168.56.202\u0026quot;, \u0026quot;phase\u0026quot;: \u0026quot;Running\u0026quot;, \u0026quot;podIP\u0026quot;: \u0026quot;10.10.1.23\u0026quot;, \u0026quot;qosClass\u0026quot;: \u0026quot;BestEffort\u0026quot;, \u0026quot;startTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot; } }  Remove below from pod-with-env.yaml\n- name: MY_NODE_NAME value: scratch  Add below Pod spec\n- name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName  Resulting Pod Yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: tea name: tea spec: containers: - env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName image: ansilh/demo-tea name: coffee-new resources: {} dnsPolicy: ClusterFirst restartPolicy: Never status: {}  Delete the running pod files\n$ kubectl delete pod tea  Create the pod with modified yaml file\n$ kubectl create -f pod-with-env.yaml  Make sure endpoint is up in service\n$ kubectl get ep tea NAME ENDPOINTS AGE tea 10.10.1.26:8080 31m  Refresh the browser page. This time you will see Node:k8s-worker-01\nLets do a cleanup on default namespace.\n$ kubectl delete --all pods $ kubectl delete --all services  Now you know - How to use export Objects in Yaml and Json format - How to access each fields using jsonpath - How to inject environmental variables to Pod - How to inject system generated fields to Pod using environmental variables\n"
},
{
	"uri": "/multi_container_pod/02-volumes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Persistent volumes When a Pod dies , all container\u0026rsquo;s contents will be destroyed and never preserved by default. Sometimes you need to store the contents persistently (for eg:- etcd pod)\nKubernetes have a Volumes filed in Pod spec , which can be used to mount a volume inside container.\nLets explain the volume specs\n$ kubectl explain pod.spec.volumes  So when you write Yaml , you have to put volumes object in spec. As we have seen , volumes type is \u0026lt;[]Object\u0026gt; ; means its an array\nSo the contents below volumes should start with a dash \u0026ldquo;-\u0026rdquo;. Name is a mandatory field , so lets write those.\nspec: volumes: - name: \u0026quot;data\u0026quot;  We will use hostPath for now\n$ kubectl explain pod.spec.volumes.hostPath KIND: Pod VERSION: v1 RESOURCE: hostPath \u0026lt;Object\u0026gt; DESCRIPTION: HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling. FIELDS: path \u0026lt;string\u0026gt; -required- Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath type \u0026lt;string\u0026gt; Type for HostPath Volume Defaults to \u0026quot;\u0026quot; More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath k8s@k8s-master-01:~$  Host path needs a path on the host , so lets add that as well to the spec\nspec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot;  This will add a volume to Pod\nNow we have to tell the pods to use it.\nIn containers specification, we have volumeMounts field which can be used to mount the volume.\n$ kubectl explain pod.spec.containers.volumeMounts KIND: Pod VERSION: v1 RESOURCE: volumeMounts \u0026lt;[]Object\u0026gt; DESCRIPTION: Pod volumes to mount into the container's filesystem. Cannot be updated. VolumeMount describes a mounting of a Volume within a container. FIELDS: mountPath \u0026lt;string\u0026gt; -required- Path within the container at which the volume should be mounted. Must not contain ':'. mountPropagation \u0026lt;string\u0026gt; mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. name \u0026lt;string\u0026gt; -required- This must match the Name of a Volume. readOnly \u0026lt;boolean\u0026gt; Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. subPath \u0026lt;string\u0026gt; Path within the volume from which the container's volume should be mounted. Defaults to \u0026quot;\u0026quot; (volume's root).  volumeMounts is \u0026lt;[]Object\u0026gt; . mountPath is required and name\nname must match the Name of a Volume\nResulting Pod spec will become ;\nspec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot; containers: - name: nginx image: nginx volumeMounts: - name: \u0026quot;data\u0026quot; mountPath: \u0026quot;/usr/share/nginx/html\u0026quot;  Lets add the basic fields to complete the Yaml and save the file as nginx.yaml\napiVersion: v1 kind: Pod metadata: name: nginx-pod01 spec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot; containers: - name: nginx image: nginx volumeMounts: - name: \u0026quot;data\u0026quot; mountPath: \u0026quot;/usr/share/nginx/html\u0026quot;  Create the Pod\nkubectl create -f nginx.yaml  Check where its running.\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod01 1/1 Running 0 55s 10.10.1.27 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Lets expose this Pod first.\n$ kubectl expose pod nginx-pod01 --port=80 --target-port=80 --type=NodePort  error: couldn't retrieve selectors via --selector flag or introspection: the pod has no labels and cannot be exposed See 'kubectl expose -h' for help and examples.  This indicates that we didn\u0026rsquo;t add label , because the service needs a label to map the Pod to endpoint\nLets add a label to the Pod.\n$ kubectl label pod nginx-pod01 run=nginx-pod01  Now we can we can expose the Pod\n$ kubectl expose pod nginx-pod01 --port=80 --target-port=80 --type=NodePort  Get the node port which service is listening to\n$ kubectl get svc nginx-pod01 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-pod01 NodePort 192.168.10.51 \u0026lt;none\u0026gt; 80:31538/TCP 26s  You will get 403 Forbidden page , because there is no html page to load.\nNow we can go to the node where the Pod is running and check the path /var/data\nk8s@k8s-worker-01:~$ ls -ld /var/data drwxr-xr-x 2 root root 4096 Jan 7 00:52 /var/data k8s@k8s-worker-01:~$ cd /var/data k8s@k8s-worker-01:/var/data$ ls -lrt total 0 k8s@k8s-worker-01:/var/data$  Nothing is there.The directory is owned by root , so you have to create the file index.html with root.\nk8s@k8s-worker-01:/var/data$ sudo -i [sudo] password for k8s: root@k8s-worker-01:~# cd /var/data root@k8s-worker-01:/var/data# root@k8s-worker-01:/var/data# echo \u0026quot;This is a test page\u0026quot; \u0026gt;index.html root@k8s-worker-01:/var/data#  Reload the web page and you should see \u0026ldquo;This is a test page\u0026rdquo;\nNow you know; - How to create a volume. - How to mount a volume. - How to access the contents of volume from host.\n"
},
{
	"uri": "/multi_container_pod/03-init-container/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " InitContainer In this session , we will discuss about InitContainer\nNon-persistent web server As we already know ,containers are ephemeral and the modifications will be lost when container is destroyed.\nIn this example , we will download webpages from Github repository and store it in a emptyDir volume.\nFrom this emptyDir volume , we will serve the HTML pages using an Nginx Pod\nemptyDir is a volume type , just like hostPath , but the contents of emptyDir will be destroyed when Pod is stopped.\nSo lets write a Pod specification for Nginx container and add InitContainer to download HTML page\napiVersion: v1 kind: Pod metadata: labels: run: demo-web name: demo-web spec: volumes: - name: html emptyDir: {} containers: - image: nginx name: demo-web volumeMounts: - name: html mountPath: /usr/share/nginx/html initContainers: - image: ansilh/debug-tools name: git-pull args: - git - clone - https://github.com/ansilh/k8s-demo-web.git - /html/. volumeMounts: - name: html mountPath: /html/  Problem with this design is , no way to pull the changes once Pod is up. InitContainer run only once during the startup of the Pod.\nIncase of InitContainer failure , Pod startup will fail and never start other containers.\nWe can specify more than one initcontainer if needed. Startup of initcontainer will be sequential and the order will be selected based on the order in yaml spec.\nIn next session , we will discuss about other design patterns for Pod.\n"
},
{
	"uri": "/pods/01-what-is-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " What is a Pod ? A Pod is the basic building block of Kubernetes–the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents a running process on your cluster\nThe “one-container-per-Pod” model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container, and Kubernetes manages the Pods rather than the containers directly.\nA Pod might encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers might form a single cohesive unit of service–one container serving files from a shared volume to the public, while a separate “sidecar” container refreshes or updates those files. The Pod wraps these containers and storage resources together as a single manageable entity.\nWhat is a Node? A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master\u0026rsquo;s automatic scheduling takes into account the available resources on each Node.\n"
},
{
	"uri": "/services/01-expose-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Expose service running on Pod. Service A Coffee Pod running in cluster and its listening on port 9090 on Pod\u0026rsquo;s IP. How can we expose that service to external world so that users can access it ?\nWe need to expose the service.\nAs we know , the Pod IP is not routable outside of the cluster. So we need a mechanism to reach the host\u0026rsquo;s port and then that traffic should be diverted to Pod\u0026rsquo;s port.\nLets create a Pod Yaml first.\n$ vi coffe.yaml  apiVersion: v1 kind: Pod metadata: name: coffee spec: containers: - image: ansilh/demo-coffee name: coffee  Create Yaml\n$ kubectl create -f coffe.yaml  Expose the Pod with below command\n$ kubectl expose pod coffee --port=80 --target-port=9090 --type=NodePort  This will create a Service object in kubernetes , which will map the Node\u0026rsquo;s port 30836 to Service IP/Port 192.168.10.86:80\nWe can see the derails using kubectl get service command\n$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE coffee NodePort 192.168.10.86 \u0026lt;none\u0026gt; 80:30391/TCP 6s kubernetes ClusterIP 192.168.10.1 \u0026lt;none\u0026gt; 443/TCP 26h  We can also see that the port is listening and kube-proxy is the one listening on that port.\n$ sudo netstat -tnlup |grep 30836 tcp6 0 0 :::30391 :::* LISTEN 2785/kube-proxy  Now you can open browser and access the Coffee app using URL http://192.168.56.201:30391\n"
},
{
	"uri": "/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Chapter 1 Introduction In this chapter we will discuss about the basic building blocks of containers with few demos and architecture of kubernetes\n"
},
{
	"uri": "/pods/02-create-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pod - Declarative Way After completing this session , you will be able to create Pod declaratively and will be able to login to check services running on other pods.\nSo lets get started.\nLets Check the running Pods k8s@k8s-master-01:~$ kubectl get pods No resources found. k8s@k8s-master-01:~$  Nothing \nLets create one using a YAML file vi pod.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee  Apply YAML using kubectl command k8s@k8s-master-01:~$ kubectl apply -f pod.yaml pod/coffee-app created k8s@k8s-master-01:~$  View status of Pod Pod status is ContainerCreating\nk8s@k8s-master-01:~$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 0/1 ContainerCreating 0 4s k8s@k8s-master-01:~$  Execute kubectl get pods after some time Now Pod status will change to Running\nk8s@k8s-master-01:~$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 27s k8s@k8s-master-01:~$  Now we can see our first Pod \nGet the IP address of Pod k8s@k8s-master-01:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee-app 1/1 Running 0 2m8s 192.168.1.7 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; k8s@k8s-master-01:~$  Create a new CentOS container $ vi centos-pod.yaml  apiVersion: v1 kind: Pod metadata: name: centos-pod spec: containers: - image: tutum/centos name: centos  Apply the Yaml spec kubectl apply -f centos-pod.yaml pod/centos-pod created  Verify the status of Pod $ kubectl get pods NAME READY STATUS RESTARTS AGE centos-pod 0/1 ContainerCreating 0 12s coffee-app 1/1 Running 0 5m31s  After some time status will change to Running $ kubectl get pods NAME READY STATUS RESTARTS AGE centos-pod 1/1 Running 0 59s coffee-app 1/1 Running 0 6m18s  Login to CentOS Pod $ kubectl exec -it centos-pod -- /bin/bash [root@centos-pod /]#  Verify Coffee app using curl [root@centos-pod /]# curl -s 192.168.1.13:9090 |grep 'Serving Coffee'90 \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\u0026lt;body\u0026gt;\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Serving Coffee from\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Pod:coffee-app\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;IP:192.168.1.13\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;Node:172.16.0.1\u0026lt;/h3\u0026gt;\u0026lt;img src=\u0026quot;data:image/png;base64, [root@centos-pod /]#  Delete pod k8s@k8s-master-01:~$ kubectl delete pod coffee-app centos-pod pod \u0026quot;coffee-app\u0026quot; deleted pod \u0026quot;centos-pod\u0026quot; deleted k8s@k8s-master-01:~$ kubectl get pods No resources found. k8s@k8s-master-01:~$  "
},
{
	"uri": "/services/02-nodeport/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " NodePort NodePort Exposes the service on each Node’s IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You’ll be able to contact the NodePort service, from outside the cluster, by requesting :.\nHow nodePort works kube-proxy watches the Kubernetes master for the addition and removal of Service and Endpoints objects.\n(We will discuss about Endpoints later in this session.)\nFor each Service, it opens a port (randomly chosen) on the local node. Any connections to this “proxy port” will be proxied to one of the Service’s backend Pods (as reported in Endpoints). Lastly, it installs iptables rules which capture traffic to the Service’s clusterIP (which is virtual) and Port and redirects that traffic to the proxy port which proxies the backend Pod.\nnodePort workflow.  nodePort -\u0026gt; 30391\n port -\u0026gt; 80\n targetPort -\u0026gt; 9090\n  "
},
{
	"uri": "/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " Chapter 2 Installation In this chapter we will install virtualbox and setup networking. We will lern how to install and configure Docker Also we will install a two node kubernete cluster using kubeadm.\n"
},
{
	"uri": "/pods/03-create-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Create Pod - Imperative Way Execute kubectl command to create a Pod. $ kubectl run coffee --image=ansilh/demo-coffee --restart=Never pod/coffee created  Verify Pod status $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee 0/1 ContainerCreating 0 6s \u0026lt;none\u0026gt; k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee 1/1 Running 0 19s 192.168.1.15 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Start a CentOS container $ kubectl run centos-pod --image=tutum/centos --restart=Never pod/centos-pod created  verify status of the Pod ; it should be in Running $ kubectl get pods NAME READY STATUS RESTARTS AGE centos-pod 1/1 Running 0 25s coffee 1/1 Running 0 2m10s  Logon to CentOS Pod $ kubectl exec -it centos-pod -- /bin/bash [root@centos-pod /]#  Verify Coffee App status [root@centos-pod /]# curl -s 192.168.1.15:9090 |grep 'Serving Coffee' \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\u0026lt;body\u0026gt;\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Serving Coffee from\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Pod:coffee\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;IP:192.168.1.15\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;Node:172.16.0.1\u0026lt;/h3\u0026gt;\u0026lt;img src=\u0026quot;data:image/png;base64, [root@centos-pod /]# exit  Delete pod k8s@k8s-master-01:~$ kubectl delete pod coffee centos-pod pod \u0026quot;coffee\u0026quot; deleted pod \u0026quot;centos-pod\u0026quot; deleted k8s@k8s-master-01:~$ kubectl get pods No resources found. k8s@k8s-master-01:~$  "
},
{
	"uri": "/services/03-clusterip/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Service with type clusterIP It exposes the service on a cluster-internal IP.\nWhen we expose a pod using kubectl expose command , we are creating a service object in API.\nChoosing this value makes the service only reachable from within the cluster. This is the default ServiceType.\nWe can see the Service spec using --dry-run \u0026amp; --output=yaml\n$ kubectl expose pod coffee --port=80 --target-port=9090 --type=ClusterIP --dry-run --output=yaml  Output\napiVersion: v1 kind: Service metadata: creationTimestamp: null labels: run: coffee name: coffee spec: ports: - port: 80 protocol: TCP targetPort: 9090 selector: run: coffee type: ClusterIP status: loadBalancer: {}  Cluster IP service is useful when you don\u0026rsquo;t want to expose the service to external world. eg:- database service.\nWith service names , a frontend tier can access the database backend without knowing the IPs of the Pods.\nCoreDNS (kube-dns) will dynamically create a service DNS entry and that will be resolvable from Pods.\nVerify Service DNS Start debug-tools container which is an alpine linux image with network related binaries\n$ kubectl run debugger --image=ansilh/debug-tools --restart=Never  $ kubectl exec -it debugger -- /bin/sh / # nslookup coffee Server: 192.168.10.10 Address: 192.168.10.10#53 Name: coffee.default.svc.cluster.local Address: 192.168.10.86 / # nslookup 192.168.10.86 86.10.168.192.in-addr.arpa name = coffee.default.svc.cluster.local. / #  coffee.default.svc.cluster.local ^ ^ ^ k8s domain | | | |-----------| | | +--------------- Indicates that its a service | +---------------------- Namespace +----------------------------- Service Name  "
},
{
	"uri": "/pods/",
	"title": "Pods &amp; Nodes",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Pods \u0026amp; Nodes In this session , we will explore Pods and Nodes.\nWe will also create a Coffee application Pod\n"
},
{
	"uri": "/pods/04-nodes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Nodes In this session , we will explore the node details\nList nodes $ k8s@k8s-master-01:~$ kubectl get nodes  Output\nNAME STATUS ROLES AGE VERSION k8s-master-01 Ready master 38h v1.13.1 k8s-worker-01 Ready \u0026lt;none\u0026gt; 38h v1.13.1  Extended listing $ kubectl get nodes -o wide  Output\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master-01 Ready master 38h v1.13.1 192.168.56.201 \u0026lt;none\u0026gt; Ubuntu 16.04.5 LTS 4.4.0-131-generic docker://18.9.0 k8s-worker-01 Ready \u0026lt;none\u0026gt; 38h v1.13.1 192.168.56.202 \u0026lt;none\u0026gt; Ubuntu 16.04.5 LTS 4.4.0-131-generic docker://18.9.0 k8s@k8s-master-01:~$  Details on a node $ kubectl describe node k8s-master-01  Output\nName: k8s-master-01 Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=k8s-master-01 node-role.kubernetes.io/master= Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 projectcalico.org/IPv4Address: 192.168.56.201/24 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 31 Dec 2018 02:10:05 +0530 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 22:59:35 +0530 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.56.201 Hostname: k8s-master-01 Capacity: cpu: 1 ephemeral-storage: 49732324Ki hugepages-2Mi: 0 memory: 2048168Ki pods: 110 Allocatable: cpu: 1 ephemeral-storage: 45833309723 hugepages-2Mi: 0 memory: 1945768Ki pods: 110 System Info: Machine ID: 96cedf74a821722b0df5ee775c291ea2 System UUID: 90E04905-218D-4673-A911-9676A65B07C5 Boot ID: 14201246-ab82-421e-94f6-ff0d8ad3ba54 Kernel Version: 4.4.0-131-generic OS Image: Ubuntu 16.04.5 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.0 Kubelet Version: v1.13.1 Kube-Proxy Version: v1.13.1 PodCIDR: 192.168.0.0/24 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system calico-node-nkcrd 250m (25%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system etcd-k8s-master-01 0 (0%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-apiserver-k8s-master-01 250m (25%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-controller-manager-k8s-master-01 200m (20%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-proxy-tzznm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-scheduler-k8s-master-01 100m (10%) 0 (0%) 0 (0%) 0 (0%) 38h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 800m (80%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%) Events: \u0026lt;none\u0026gt;  We will discuss more about each of the fields on upcoming sessions. For now lets discuss about Non-terminated Pods field;\nNon-terminated Pods field  Namespace : The namespace which the Pods were running . The pods that we create will by default go to default namespace. Name : Name of the Pod CPU Request : How much CPU resource requested by Pod during startup. CPU Limits : How much CPU the Pod can use. Memory Request : How much memory requested by Pod during startup. Memory Limits : How much memory the Pod can use.  "
},
{
	"uri": "/services/04-loadbalancer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Load Balancer Exposes the service externally using a cloud provider’s load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created.\nWe will discuss more about this topic later in this training.\n"
},
{
	"uri": "/labels_and_annotations/",
	"title": "Labels &amp; Annotations",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Labels \u0026amp; Annotations In this session , we will discuss the role of Labels and Annotations , also its role in fundamental k8s design.\n"
},
{
	"uri": "/installation/01-vbox-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " VirtualBox  Download VBox installer Download VBox Extension Pack\n https://download.virtualbox.org/virtualbox/LATEST.TXT https://download.virtualbox.org/virtualbox//  Procedure is available in below link\n https://www.wikihow.com/Install-VirtualBox   "
},
{
	"uri": "/introduction/01-linux-kernel/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Linux Kernel Architecture At the top is the user, or application, space. This is where the user applications are executed. Below the user space is the kernel space. Here, the Linux kernel exists.\nThere is also the GNU C Library (glibc). This provides the system call interface that connects to the kernel and provides the mechanism to transition between the user-space application and the kernel. This is important because the kernel and user application occupy different protected address spaces. And while each user-space process occupies its own virtual address space, the kernel occupies a single address space.\nThe Linux kernel can be further divided into three gross levels. At the top is the system call interface, which implements the basic functions such as read and write. Below the system call interface is the kernel code, which can be more accurately defined as the architecture-independent kernel code. This code is common to all of the processor architectures supported by Linux. Below this is the architecture-dependent code, which forms what is more commonly called a BSP (Board Support Package). This code serves as the processor and platform-specific code for the given architecture.\nThe Linux kernel implements a number of important architectural attributes. At a high level, and at lower levels, the kernel is layered into a number of distinct subsystems.\n"
},
{
	"uri": "/pods/05-namespaces/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Namespaces What is a namespace We have see namespaces in Linux , which ideally isolates objects and here also the concept is same but serves a different purpose. Suppose you have two departments in you organization and both departments have application which needs more fine grained control. We can use namespaces to separate the workload of each departments.\nBy default kubernetes will have three namespace\nList namespace $ kubectl get ns NAME STATUS AGE default Active 39h kube-public Active 39h kube-system Active 39h  default : All Pods that we manually create will go to this namespace (There are ways to change it , but for now that is what it is). kube-public : All common workloads can be assigned to this namespace . Most of the time no-one use it. kube-system : Kubernetes specific Pods will be running on this namespace\nList Pods in kube-system namespace $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE calico-node-n99tb 2/2 Running 0 38h calico-node-nkcrd 2/2 Running 0 38h coredns-86c58d9df4-4c22l 1/1 Running 0 39h coredns-86c58d9df4-b49c2 1/1 Running 0 39h etcd-k8s-master-01 1/1 Running 0 39h kube-apiserver-k8s-master-01 1/1 Running 0 39h kube-controller-manager-k8s-master-01 1/1 Running 0 39h kube-proxy-s6hc4 1/1 Running 0 38h kube-proxy-tzznm 1/1 Running 0 39h kube-scheduler-k8s-master-01 1/1 Running 0 39h  As you can see , there are many Pods running in kube-system namespace All these Pods were running with one or mode containers If you see the calico-node-n99tb pod , the READY says 2\u0026frasl;2 , which means two containers were running fine in this Pod\nList all resources in a namespace k8s@k8s-master-01:~$ kubectl get all -n kube-system NAME READY STATUS RESTARTS AGE pod/calico-node-kr5xg 2/2 Running 0 13m pod/calico-node-lcpbw 2/2 Running 0 13m pod/coredns-86c58d9df4-h8pjr 1/1 Running 6 26m pod/coredns-86c58d9df4-xj24c 1/1 Running 6 26m pod/etcd-k8s-master-01 1/1 Running 0 26m pod/kube-apiserver-k8s-master-01 1/1 Running 0 26m pod/kube-controller-manager-k8s-master-01 1/1 Running 0 26m pod/kube-proxy-fl7rj 1/1 Running 0 26m pod/kube-proxy-q6w9l 1/1 Running 0 26m pod/kube-scheduler-k8s-master-01 1/1 Running 0 26m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/calico-typha ClusterIP 172.16.244.140 \u0026lt;none\u0026gt; 5473/TCP 13m service/kube-dns ClusterIP 172.16.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 27m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/calico-node 2 2 2 2 2 beta.kubernetes.io/os=linux 13m daemonset.apps/kube-proxy 2 2 2 2 2 \u0026lt;none\u0026gt; 27m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/calico-typha 0/0 0 0 13m deployment.apps/coredns 2/2 2 2 27m NAME DESIRED CURRENT READY AGE replicaset.apps/calico-typha-5fc4874c76 0 0 0 13m replicaset.apps/coredns-86c58d9df4 2 2 2 26m k8s@k8s-master-01:~$  "
},
{
	"uri": "/services/05-endpoints/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pods behind a service. Lets describe the service to see how the mapping of Pods works in a service object.\n(Yes , we are slowly moving from general wordings to pure kubernetes terms)\n$ kubectl describe service coffee Name: coffee Namespace: default Labels: run=coffee Annotations: \u0026lt;none\u0026gt; Selector: run=coffee Type: NodePort IP: 192.168.10.86 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 9090/TCP NodePort: \u0026lt;unset\u0026gt; 30391/TCP Endpoints: 10.10.1.13:9090 Session Affinity: None External Traffic Policy: Cluster  Here the label run=coffee is the one which creates the mapping from service to Pod.\nAny pod with label run=coffee will be mapped under this service.\nThose mappings are called Endpoints.\nLets see the endpoints of service coffee\n$ kubectl get endpoints coffee NAME ENDPOINTS AGE coffee 10.10.1.13:9090 3h48m  As of now only one pod endpoint is mapped under this service.\nlets create one more Pod with same label and see how it affects endpoints.\n$ kubectl run coffee01 --image=ansilh/demo-coffee --restart=Never --labels=run=coffee  Now we have one more Pod\n$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee 1/1 Running 0 15h coffee01 1/1 Running 0 6s  Lets check the endpoint\n$ kubectl get endpoints coffee NAME ENDPOINTS AGE coffee 10.10.1.13:9090,10.10.1.19:9090 3h51m  Now we have two Pod endpoints mapped to this service. So the requests comes to coffee service will be served from these pods in a round robin fashion.\n"
},
{
	"uri": "/yaml_primer/",
	"title": "YAML Crash course",
	"tags": [],
	"description": "",
	"content": " Chapter 5 YAML crash course In this session we will learn k8s YAML specification and object types. We will cover only k8s dependent YAML specification\n"
},
{
	"uri": "/installation/02-nw-setup/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " VirtualBox network configuration  Create HostOnly network ( Default will be 192.168.56.0/24)  Open Virtual Box Got to menu and navigate to File -\u0026gt;Host Network Manager Then click \u0026ldquo;Create\u0026rdquo; This will create a Host-Only Network.   DHCP should be disabled on this network.\nInternet access is needed on all VMs (for downloading needed binaries).\nMake sure you can see the NAT network.(If not , create one).\n   VBox Host Networking      HostOnly 192.168.56.0/24   NAT VBOX Defined    "
},
{
	"uri": "/introduction/02-namespaces/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Namespaces in Linux Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources. The feature works by having the same name space for these resources in the various sets of processes, but those names referring to distinct resources. Examples of resource names that can exist in multiple spaces, so that the named resources are partitioned, are process IDs, hostnames, user IDs, file names, and some names associated with network access, and interprocess communication.\nNamespaces are a fundamental aspect of containers on Linux.\n   Namespace Constant Isolates     Cgroup CLONE_NEWCGROUP Cgroup root directory   IPC CLONE_NEWIPC System V IPC, POSIX message queues   Network CLONE_NEWNET Network devices, stacks, ports, etc.   Mount CLONE_NEWNS Mount points   PID CLONE_NEWPID Process IDs   User CLONE_NEWUSER User and group IDs   UTS CLONE_NEWUTS Hostname and NIS domain name    The kernel assigns each process a symbolic link per namespace kind in /proc/\u0026lt;pid\u0026gt;/ns/. The inode number pointed to by this symlink is the same for each process in this namespace. This uniquely identifies each namespace by the inode number pointed to by one of its symlinks.\nReading the symlink via readlink returns a string containing the namespace kind name and the inode number of the namespace.\n"
},
{
	"uri": "/pods/06-self-healing-readiness/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Self Healing - Readiness Readiness Probe We have seen that our coffee application was listening on port 9090. Lets assume that the application is not coming up but Pod status showing running. Everyone will think that application is up. You entire application stack might get affected because of this.\nSo here comes the question , \u0026ldquo;How can I make sure my application is started, not just the Pod ?\u0026rdquo;\nHere we can use Pod spec, Readiness probe.\nOfficial detention of readinessProbe is , \u0026ldquo;Periodic probe of container service readiness\u0026rdquo;.\nLets rewrite the Pod specification of Coffee App and add a readiness Probe.\n$ vi pod-readiness.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee readinessProbe: initialDelaySeconds: 10 httpGet: port: 9090  Apply Yaml $ kubectl apply -f pod-readiness.yaml pod/coffee-app created  Verify Pod status Try to identify the difference.\n$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 0/1 ContainerCreating 0 3s $ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 0/1 Running 0 25s $ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 32s  Delete the Pod Yes ,we can delete the objects using the same yaml which we used to create/apply it\n$ kubectl delete -f pod-readiness.yaml pod \u0026quot;coffee-app\u0026quot; deleted $  Probe Tuning. failureThreshold \u0026lt;integer\u0026gt; Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. initialDelaySeconds \u0026lt;integer\u0026gt; Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes periodSeconds \u0026lt;integer\u0026gt; How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. timeoutSeconds \u0026lt;integer\u0026gt; Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes  "
},
{
	"uri": "/services/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": " Chapter 6 Services and Service Discovery In this session we will solve the maze "
},
{
	"uri": "/installation/03-ubuntu-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Ubuntu installation  Download Ubuntu 16.04 ISO http://releases.ubuntu.com/16.04/ubuntu-16.04.5-server-amd64.iso  Create a template VM which will be used to clone all needed VMs\n You need at least 50GB free space to host all VMs All VMs will be placed in a directory called (Don\u0026rsquo;t create these manually now!) DRIVE_NAME:/VMs/ (Replace DRIVE_NAME with a mount point or Driver name) Install Ubuntu 16.04 with latest patches VM configuration\n VM Name : k8s-master-01 Memory : 2 GB CPU : 2 Disk : 100GB HostOnly interface : 1 (ref. step 1). NAT network interface : 1   By default , NAT will be the first in network adapter order , change it like below. NAT interface should be the second interface Host-Only should be the first one\n Install Ubuntu on this VM and go ahead with all default options\n When asked, provide user name k8s and set password Select below in Software Selection screen Manual Software Selection OpenSSH Server  After restart , make sure NAT interface is up\n Login to the template VM with user k8s and execute below commands to install latest patches.\n$ sudo apt-get update $ sudo apt-get upgrade   Poweroff template VM shell $ sudo poweroff   Open CMD and execute below commands to create all needed VMs. You can replace the value of DRIVER_NAME with a drive which is having enough free space (~50GB)\n Windows\nset DRIVE_NAME=D cd C:\\Program Files\\Oracle\\VirtualBox VBoxManage.exe clonevm \u0026quot;k8s-master-01\u0026quot; --name \u0026quot;k8s-worker-01\u0026quot; --groups \u0026quot;/K8S Training\u0026quot; --basefolder \u0026quot;%DRIVE_NAME%:\\VMs\u0026quot; --register   Mac or Linux (Need to test) shell DRIVE_NAME=${HOME} VBoxManage clonevm \u0026quot;k8s-master-01\u0026quot; --name \u0026quot;k8s-worker-01\u0026quot; --groups \u0026quot;/K8S Training\u0026quot; --basefolder ${DRIVE_NAME}/VMs\u0026quot; --register    Start VMs one by one and perform below  IP Address and Hostname for each VMs\n192.168.56.201 k8s-master-01 192.168.56.202 k8s-worker-01   Assign IP address and make sure it comes up at boot time. shell $ sudo systemctl stop networking $ sudo vi /etc/network/interfaces    auto enp0s3 iface enp0s3 inet static address 192.168.56.X #\u0026lt;--- Replace X with corresponding IP octect netmask 255.255.255.0  $ sudo systemctl restart networking   You may access the VM using the IP via SSH and can complete all remaining steps from that session (for copy paste :) ) Change Host name\n$ HOST_NAME=\u0026lt;host name\u0026gt; # \u0026lt;--- Replace \u0026lt;host name\u0026gt; with corresponding one $ sudo hostnamectl set-hostname ${HOST_NAME} --static --transient   Regenrate SSH Keys shell $ sudo /bin/rm -v /etc/ssh/ssh_host_* $ sudo dpkg-reconfigure openssh-server   Change iSCSI initiator IQN\n$ sudo vi /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1993-08.org.debian:01:HOST_NAME #\u0026lt;--- Append HostName to have unique iscsi iqn   Change Machine UUID shell $ sudo rm /etc/machine-id /var/lib/dbus/machine-id $ sudo systemd-machine-id-setup   Remove 127.0.1.1 entry from /etc/hosts\n Add needed entries in /etc/hosts\n$ sudo bash -c \u0026quot;cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/hosts 192.168.56.201 k8s-master-01 192.168.56.202 k8s-worker-01 EOF\u0026quot;   Add public DNS incase the local one is not responding in NAT bash $ sudo bash -c \u0026quot;cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/resolvconf/resolv.conf.d/tail nameserver 8.8.8.8 EOF\u0026quot;   Disable swap by commenting out swap_1 LV\n$ sudo vi /etc/fstab  # /dev/mapper/k8s--master--01--vg-swap_1 none swap sw 0 0   Reboot VM shell $ sudo reboot   Repeat the steps above for second VM\n Do a ping test to make sure all VMs can reach each other.\n  "
},
{
	"uri": "/introduction/03-cgroups/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " CGroups cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.\nResource limiting groups can be set to not exceed a configured memory limit\nPrioritization Some groups may get a larger share of CPU utilization or disk I/O throughput\nAccounting Measures a group\u0026rsquo;s resource usage, which may be used\nControl Freezing groups of processes, their checkpointing and restarting\nYou can read and explore more about cGroups in this post\n"
},
{
	"uri": "/pods/07-self-healing-liveness/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Self Healing - Liveness Liveness Probe Lets assume the application failed after readiness probe execution completes Again we are back to service unavailability\nTo avoid this , we need a liveness check which will do a periodic health check after Pod start running or readiness probe completes.\nLets rewrite the Pod specification of Coffee App and add a liveness Probe.\n$ vi pod-liveiness.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee readinessProbe: initialDelaySeconds: 10 httpGet: port: 9090 livenessProbe: periodSeconds: 5 httpGet: port: 9090  Create Pod $ kubectl create -f pod-liveness.yaml  "
},
{
	"uri": "/yaml_primer/01-structure/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " K8S YAML structure What is YAML Yet Another Markup Language\nKubernetes YAML have below structure\napiVersion: kind: metadata: spec:  apiVersion: Kubernetes have different versions of API for each objects. We discuss about API in detail in upcoming sessions. For now , lets keep it simple as possible.\nPod is one of the kind of object which is part of core v1 API So for a Pod, we usually see apiVersion: v1\nkind: As explained above we specify the kind of API object with kind: field.\nmetadata: We have seen the use of metadata earlier.\nAs the name implies , we usually store name of object and labels in metadata field.\nspec: Object specification will go hear. The specification will depend on the kind and apiVersion we use\nExploring Pod spec Lets write a Pod specification YAML\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee  In above specification , you can see that we have specified name and labels in matadata field.\nThe spec starts with cotainer field and we have added a container specification under it.\nYou might be wondering , how can we memories all these options. In reality , you don\u0026rsquo;t have to.\nWe will discuss about it in next session.\n"
},
{
	"uri": "/yaml_primer/02-explore/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Exploring Object Specs So lets discuss about a new command kubectl explore so that we don\u0026rsquo;t have to remember all YAML specs of kubernetes objects.\nWith kubectl explore subcommand , you can see the specification of each objects and can use that as a reference to write your YAML files.\nFist level spec We will use kubectl explore Pod command to see the specifications of a Pod YAML.\n$ kubectl explore Pod  Output\nubuntu@k8s-master-01:~$ kubectl explain pod KIND: Pod VERSION: v1 DESCRIPTION: Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts. FIELDS: apiVersion \u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind \u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata \u0026lt;Object\u0026gt; Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec \u0026lt;Object\u0026gt; Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status status \u0026lt;Object\u0026gt; Most recently observed status of the pod. This data may not be up to date. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status ubuntu@k8s-master-01:~$  As we discussed earlier , the specification is very familiar.\nFiled status is readonly and its system populated , so we don\u0026rsquo;t have to write anything for status.\nExploring inner fields If we want to see the fields available in spec , then execute below command.\n$ kubectl explain pod.spec  KIND: Pod VERSION: v1 RESOURCE: spec \u0026lt;Object\u0026gt; DESCRIPTION: Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status PodSpec is a description of a pod. FIELDS: ... containers \u0026lt;[]Object\u0026gt; -required- List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. ...  How easy is that.\nAs you can see in spec the containers filed is -required- which indicates that this filed is mandatory.\n\u0026lt;[]Object\u0026gt; indicates that its an array of objects , which means , you can put more than one element under containers\nThat make sense , because the Pod may contain more than one container.\nIn YAML we can use - infront of a filed to mark it as an array element.\nLets take a look at the YAML that we wrote earlier\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee  There is a - under the fist filed of the containers. If we say that in words ; \u0026ldquo;containers is an array object which contains one array element with filed name and image\u0026ldquo;\nIf you want to add one more container in Pod , we will add one more array element with needed values.\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee - name: demo-tea image: ansilh/demo-tea  Now the Pod have two containers .\nHow I know the containers array element need name and image ?\nWe will use explore command to get those details.\n$ kubectl explain pod.spec.containers  Snipped Output\n... name \u0026lt;string\u0026gt; -required- Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. image \u0026lt;string\u0026gt; Docker image name ...  As you can see , name and image are of type string which means , you have to provide a string value to it.\n"
},
{
	"uri": "/multi_container_pod/",
	"title": "Multi-Container Pods",
	"tags": [],
	"description": "",
	"content": " Chapter 7 Multi-Container Pods In this session we will create Pods with more than one containers and few additional features in k8s.\n"
},
{
	"uri": "/installation/04-docker-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Install Docker In this session, we will install and setup docker in a simple and easy way on Ubuntu 16.04.\nAdd gpg key to aptitude $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -  Add repository $ sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot;  Refresh repository $ sudo apt-get update  Verify whether docker is available in repo or not $ sudo apt-cache policy docker-ce  docker-ce: Installed: (none) Candidate: 5:18.09.0~3-0~ubuntu-xenial Version table: 5:18.09.0~3-0~ubuntu-xenial 500 ...  Install docker $ sudo apt-get install -y docker-ce  Make sure docker is running $ sudo systemctl status docker  ● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-12-26 17:14:59 UTC; 4min 27s ago Docs: https://docs.docker.com Main PID: 1191 (dockerd) Tasks: 10 Memory: 76.4M CPU: 625ms CGroup: /system.slice/docker.service └─1191 /usr/bin/dockerd -H unix:// ...  Add user to docker group so that this user can execute docker commands. $ sudo usermod -aG docker ${USER}  Logout the session and login again to refresh the group membership.\nVerify docker by executing info command. $ docker info |grep 'Server Version'  Server Version: 18.09.0  "
},
{
	"uri": "/introduction/04-containers/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container From Scratch Using namespaces , we can start a process which will be completely isolated from other processes running in the system.\nCreate root File System Create directory to store rootfs contents $ mkdir -p /root/busybox/rootfs $ CONTAINER_ROOT=/root/busybox/rootfs $ cd ${CONTAINER_ROOT}  Download busybox binary $ wget https://busybox.net/downloads/binaries/1.28.1-defconfig-multiarch/busybox-x86_64  Create needed directories and symlinks $ mv busybox-x86_64 busybox $ chmod 755 busybox $ mkdir bin $ mkdir proc $ mkdir sys $ mkdir tmp $ for i in $(./busybox --list) do ln -s /busybox bin/$i done  Start Container Start a shell in new contianer $ unshare --mount --uts --ipc --net --pid --fork --user --map-root-user chroot ${CONTAINER_ROOT} /bin/sh  Mount essential kernel structures $ mount -t proc none /proc $ mount -t sysfs none /sys $ mount -t tmpfs none /tmp  Configure networking From Host system , create a veth pair and then map that to container $ sudo ip link add vethlocal type veth peer name vethNS $ sudo ip link set vethlocal up $ sudo ip link set vethNS up $ sudo ps -ef |grep '/bin/sh' $ sudo ip link set vethNS netns \u0026lt;pid of /bin/sh\u0026gt;  From container , execute ip link "
},
{
	"uri": "/pods/08-request-limits/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Resource Allocation - CPU and Memory allocation for containers. Limits We can limit the CPU and Memory usage of a container so that one\nLets create the coffee Pod again with CPU and Memory limits\napiVersion: v1 kind: Pod metadata: labels: name: coffee-limits spec: containers: - image: ansilh/demo-coffee name: coffee resources: limits: CPU: 100m Memory: 123Mi  Resulting container will be allowed to use 100 millicores and 123 mebibyte (~128 Megabytes)\nCPU One CPU core is equivalent to 1000m (one thousand millicpu or one thousand millicores) CPU is always expressed as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine\nMemory You can express memory as a plain integer or as a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:\n128974848, 129e6, 129M, 123Mi  Mebibyte vs Megabyte 1 Megabyte (MB) = (1000)^2 bytes = 1000000 bytes. 1 Mebibyte (MiB) = (1024)^2 bytes = 1048576 bytes.  Requests We can request a specific amount of CPU and Memory when the container starts up.\nSuppose if the Java application need at least 128MB of memory during startup , we can use resource request in Pod spec.\nThis will help the scheduler to select a node with enough memory.\nRequest also can be made of CPU as well.\nLets modify the Pod spec and add request\napiVersion: v1 kind: Pod metadata: labels: name: coffee-limits spec: containers: - image: ansilh/demo-coffee name: coffee resources: requests: CPU: 100m Memory: 123Mi limits: CPU: 200m Memory: 244Mi  Extra Once you complete the training , you can visit below URLs to understand storage and network limits.\nStorage Limit\nNetwork bandwidth usage\n"
},
{
	"uri": "/installation/05-golang-setup/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Setup Golang Download Golang tarball $ curl -O https://dl.google.com/go/go1.11.4.linux-amd64.tar.gz  Extract the contents $ tar -xvf go1.11.4.linux-amd64.tar.gz  Move the contents to /usr/local directory $ sudo mv go /usr/local/  Add the environmental variable GOPATH to .profile cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;~/.profile export GOPATH=\\$HOME/work export PATH=\\$PATH:/usr/local/go/bin:\\$GOPATH/bin EOF  Create the work directory $ mkdir $HOME/work  Load the profile $ source ~/.profile  Verify Golang setup $ go version  go version go1.11.4 linux/amd64  Create a directory tree to map to a github repository $ mkdir -p $GOPATH/src/github.com/ansilh/golang-demo  Create a hello world golang program $ vi $GOPATH/src/github.com/ansilh/golang-demo/main.go  Paste below code package main import \u0026quot;fmt\u0026quot; func main(){ fmt.Println(\u0026quot;Hello World.!\u0026quot;) }  Build and install the program go install github.com/ansilh/golang-demo  Execute the program to see the output $ golang-demo  Hello World.!  "
},
{
	"uri": "/introduction/07-network-plugins/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container networking - Demo We need to access the container from outside world and the container running on different hosts have to communicate each other.\nHere we will see how can we do it with bridging.\nTraditional networking Create a veth pair on Host. $ sudo ip link add veth0 type veth peer name veth1 $ sudo ip link show  Create a network namespace $ sudo ip netns add bash-nw-namespace $ sudo ip netns show  Connect one end to namespace $ sudo ip link set veth1 netns bash-nw-namespace $ sudo ip link list  Resulting network Create a Bridge interface $ sudo brctl addbr cbr0  Add an external interface to bridge $ sudo brctl addif cbr0 enp0s9 $ sudo brctl show  Connect other end to a switch $ sudo brctl addif cbr0 veth0 $ sudo brctl show  Resulting network Assign IP to interface $ sudo ip netns exec bash-nw-namespace bash $ sudo ip addr add 192.168.56.10/24 dev veth1 $ sudo ip link set lo up $ sudo ip link set dev veth1 up  Access container IP from outside Like bridging , we can opt other networking solutions.\nLater we will see how Weave Network and Calico plugins works. You may read bit more on Docker networking basics on below blog post\nDocker networking\n"
},
{
	"uri": "/installation/06-demo-webapp/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Build a Demo WebApp Create a directory for the demo app. $ mkdir -p ${GOPATH}/src/github.com/ansilh/demo-webapp  Create demo-webapp.go file $ vi ${GOPATH}/src/github.com/ansilh/demo-webapp/demo-webapp.go  package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;log\u0026quot; ) func demoDefault(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;404 - Page not found - This is a dummy default backend\u0026quot;) // send data to client side } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, demoDefault) // set router err := http.ListenAndServe(\u0026quot;:9090\u0026quot;, nil) // set listen port if err != nil { log.Fatal(\u0026quot;ListenAndServe: \u0026quot;, err) } }  Build a static binary $ cd $GOPATH/src/github.com/ansilh/demo-webapp $ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -ldflags=\u0026quot;-w -s\u0026quot; -o $GOPATH/bin/demo-webapp  Execute the program $ demo-webapp  Open the browser and check if you can see the response using IP:9090 If you see the output “404 – Page not found – This is a dummy default backend” indicates that the program is working\nPress Ctrl+c to terminate the program\n"
},
{
	"uri": "/introduction/05-docker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " What is Docker Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.\nIn a way, Docker is a bit like a virtual machine. But unlike a virtual machine, rather than creating a whole virtual operating system, Docker allows applications to use the same Linux kernel as the system that they\u0026rsquo;re running on and only requires applications be shipped with things not already running on the host computer. This gives a significant performance boost and reduces the size of the application.\n"
},
{
	"uri": "/installation/07-build-docker-image/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Build Docker image using Dockerfile Create a Docker Hub account\nLet’s create a directory to store the Dockerfile $ mkdir ~/demo-webapp  Copy the pre-built program $ cp $GOPATH/bin/demo-webapp ~/demo-webapp/  Create a Dockerfile. $ cd ~/demo-webapp/ $ vi Dockerfile  FROM scratch LABEL maintainer=\u0026quot;Ansil H\u0026quot; LABEL email=\u0026quot;ansilh@gmail.com\u0026quot; COPY demo-webapp / CMD [\u0026quot;/demo-webapp\u0026quot;]  Build the docker image $ sudo docker build -t \u0026lt;docker login name\u0026gt;/demo-webapp . Eg:- $ sudo docker build -t ansilh/demo-webapp .  Login to Docker Hub using your credentials $ docker login  Push image to Docker hub $ docker push \u0026lt;docker login name\u0026gt;/demo-webapp Eg:- $ docker push ansilh/demo-webapp  Congratulations ! . Now the image you built is available in Docker Hub and we can use this image to run containers in upcoming\n"
},
{
	"uri": "/introduction/06-kubernetes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Kubernetes Pet vs Cattle\nIn the pets service model, each pet server is given a loving names like zeus, ares, hades, poseidon, and athena. They are “unique, lovingly hand-raised, and cared for, and when they get sick, you nurse them back to health”. You scale these up by making them bigger, and when they are unavailable, everyone notices.\nIn the cattle service model, the servers are given identification numbers like web-01, web-02, web-03, web-04, and web-05, much the same way cattle are given numbers tagged to their ear. Each server is “almost identical to each other” and “when one gets sick, you replace it with another one”. You scale these by creating more of them, and when one is unavailable, no one notices.\nKubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nGoogle open-sourced the Kubernetes project in 2014. Kubernetes builds upon a decade and a half of experience that Google has with running production workloads at scale, combined with best-of-breed ideas and practices from the community\nRead More here\nKubernetes Architecture Container runtime Docker , rkt , containerd or any OCI compliant runtime which will download image , configures network , mount volumes and assist container life cycle management.\nkubelet Responsible for instructing container runtime to start , stop or modify a container\nkube-proxy Manage service IPs and iptables rules\nkube-apiserver API server interacts with all other components in cluster All client interactions will happen via API server\nkube-scheduler Responsible for scheduling workload on minions or worker nodes based on resource constraints\nkube-controller-manager Responsible for monitoring different containers in reconciliation loop Will discuss more about different controllers later in this course\netcd Persistent store where we store all configurations and cluster state\ncloud-controller-manager Cloud vendor specific controller and cloud vendor is Responsible to develop this program\n"
},
{
	"uri": "/installation/08-run-docker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container management using Docker Start a Container  Here we map port 80 of host to port 9090 of cotainer Verify application from browser Press Ctrl+c to exit container  $ docker run -p 80:9090 ansilh/demo-webapp  Start a Container in detach mode $ docker run -d -p 80:9090 ansilh/demo-webapp  List Container $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c8364e0d031 ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 11 seconds ago Up 10 seconds 0.0.0.0:80-\u0026gt;9090/tcp zen_gauss  List all containers including stopped containers $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c8364e0d031 ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 2 minutes ago Up 2 minutes 0.0.0.0:80-\u0026gt;9090/tcp zen_gauss acb01851c20a ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 2 minutes ago Exited (2) 2 minutes ago condescending_antonelli  List resource usage (Press Ctrl+c to exit) $ docker stats zen_gauss  Stop Container $ docker stop zen_gauss  List images $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ansilh/demo-webapp latest b7c5e17ae85e 8 minutes ago 4.81MB  Remove containers $ docker rm zen_gauss  Delete images $ docker rmi ansilh/demo-webapp  "
},
{
	"uri": "/installation/09-install-kubeadm/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Install kubelet , kubeadm and kubectl Verify the MAC address and product_uuid are unique for every node  ip link or ifconfig -a sudo cat /sys/class/dmi/id/product_uuid  Download pre-requisites $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl  Add gpg key for apt $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg |sudo apt-key add -  Add apt repository $ cat \u0026lt;\u0026lt;EOF |sudo tee -a /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF  Install kubelet , kubeadm and kubectl $ sudo apt-get update $ sudo apt-get install -y kubelet kubeadm kubectl $ sudo apt-mark hold kubelet kubeadm kubectl  Repeat the same steps on worker node\n"
},
{
	"uri": "/installation/10-master/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Deploy master Node (k8s-master-01) Initialize kubeadm with pod IP range $ sudo kubeadm init --apiserver-advertise-address=192.168.56.201 --pod-network-cidr=10.10.0.0/16 --service-cidr=192.168.10.0/24  Configure kubectl $ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config  Verify master node status $ kubectl cluster-info  Output will be like below Kubernetes master is running at https://192.168.56.201:6443 KubeDNS is running at https://192.168.56.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  Move to next session to deploy network plugin. "
},
{
	"uri": "/installation/11-network-plugin/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Deploy Network Plugin - Calico Apply RBAC rules (More about RBAC will discuss later) $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml  Download Calico deployment YAML wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml  Edit CALICO_IPV4POOL_CIDR value to 192.168.10.0/24 (To avoid node IP range conflict) Change\n- name: CALICO_IPV4POOL_CIDR value: \u0026quot;192.168.10.0/24\u0026quot;  Add name: IP_AUTODETECTION_METHOD \u0026amp; value: \u0026quot;can-reach=192.168.56.1\u0026quot; (This IP should be the host only network ip on your laptop) ... image: quay.io/calico/node:v3.3.2 env: - name: IP_AUTODETECTION_METHOD value: \u0026quot;can-reach=192.168.56.1\u0026quot; ...  Apply Deployment $ kubectl apply -f calico.yaml  Make sure the READY status should show same value on left and right side of / and Pod STATUS should be Running $ kubectl get pods -n kube-system |nl  1 NAME READY STATUS RESTARTS AGE 2 calico-node-2pwv9 2/2 Running 0 20m 3 coredns-86c58d9df4-d9q2l 1/1 Running 0 21m 4 coredns-86c58d9df4-rwv7r 1/1 Running 0 21m 5 etcd-k8s-master-01 1/1 Running 0 20m 6 kube-apiserver-k8s-master-01 1/1 Running 0 20m 7 kube-controller-manager-k8s-master-01 1/1 Running 0 20m 8 kube-proxy-m6m9n 1/1 Running 0 21m 9 kube-scheduler-k8s-master-01 1/1 Running 0 20m  Contact the Trainer if the output is not the expected one after few minutes (~3-4mins). "
},
{
	"uri": "/installation/12-worker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Add worker node to cluster Get discovery secret from Master node. $ echo sha256:$(openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -pubkey | openssl rsa -pubin -outform DER 2\u0026gt;/dev/null | sha256sum | cut -d' ' -f1)  Get node join token from Master node. $ kubeadm token list |grep bootstra |awk '{print $1}'  Execute kubeadm command to add the Worker to cluster $ sudo kubeadm join 192.168.56.201:6443 --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash \u0026lt;discovery hash\u0026gt;  Verify system Pod status $ kubectl get pods -n kube-system |nl  Output 1 NAME READY STATUS RESTARTS AGE 2 calico-node-2pwv9 2/2 Running 0 20m 3 calico-node-hwnfh 2/2 Running 0 19m 4 coredns-86c58d9df4-d9q2l 1/1 Running 0 21m 5 coredns-86c58d9df4-rwv7r 1/1 Running 0 21m 6 etcd-k8s-master-01 1/1 Running 0 20m 7 kube-apiserver-k8s-master-01 1/1 Running 0 20m 8 kube-controller-manager-k8s-master-01 1/1 Running 0 20m 9 kube-proxy-m6m9n 1/1 Running 0 21m 10 kube-proxy-shwgp 1/1 Running 0 19m 11 kube-scheduler-k8s-master-01 1/1 Running 0 20m  "
},
{
	"uri": "/multi_container_pod/04-pod-patterns/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " When the containers have the exact same lifecycle, or when the containers must run on the same node. The most common scenario is that you have a helper process that needs to be located and managed on the same node as the primary container.\nAnother reason to combine containers into a single pod is for simpler communication between containers in the pod. These containers can communicate through shared volumes (writing to a shared file or directory) and through inter-process communication (semaphores or shared memory).\nThere are three common design patterns and use-cases for combining multiple containers into a single pod. We’ll walk through the sidecar pattern, the adapter pattern, and the ambassador pattern.\nExample #1: Sidecar containers Sidecar containers extend and enhance the “main” container, they take existing containers and make them better. As an example, consider a container that runs the Nginx web server. Add a different container that syncs the file system with a git repository, share the file system between the containers and you have built Git push-to-deploy.\napiVersion: v1 kind: Pod metadata: labels: run: demo-web name: demo-web spec: volumes: - name: html emptyDir: {} containers: - image: nginx name: demo-web volumeMounts: - name: html mountPath: /usr/share/nginx/html - image: ansilh/debug-tools name: git-pull args: - sh - -c - '[ ! -d /html/.git ] \u0026amp;\u0026amp; git clone https://github.com/ansilh/k8s-demo-web.git /html/ || { cd /html; git pull; } ' volumeMounts: - name: html mountPath: /html/  Lets do a tail on the logs and see how the git-pull works\n$ kubectl logs demo-web git-pull -f Cloning into '/html'... Fri Jan 11 20:39:25 UTC 2019 Already up to date. Fri Jan 11 20:39:31 UTC 2019  Lets modify the WebPage and push the changes to Github\nAlready up to date. Fri Jan 11 20:44:04 UTC 2019 From https://github.com/ansilh/k8s-demo-web e2df24f..1791ee1 master -\u0026gt; origin/master Updating e2df24f..1791ee1 Fast-forward images/pic-k8s.jpg | Bin 0 -\u0026gt; 14645 bytes index.html | 4 ++-- 2 files changed, 2 insertions(+), 2 deletions(-) create mode 100644 images/pic-k8s.jpg Fri Jan 11 20:44:10 UTC 2019 Already up to date.  Example #2: Ambassador containers Ambassador containers proxy a local connection to the world. As an example, consider a Redis cluster with read-replicas and a single write master. You can create a Pod that groups your main application with a Redis ambassador container. The ambassador is a proxy is responsible for splitting reads and writes and sending them on to the appropriate servers. Because these two containers share a network namespace, they share an IP address and your application can open a connection on “localhost” and find the proxy without any service discovery. As far as your main application is concerned, it is simply connecting to a Redis server on localhost. This is powerful, not just because of separation of concerns and the fact that different teams can easily own the components, but also because in the development environment, you can simply skip the proxy and connect directly to a Redis server that is running on localhost.\nExample #3: Adapter containers Adapter containers standardize and normalize output. Consider the task of monitoring N different applications. Each application may be built with a different way of exporting monitoring data. (e.g. JMX, StatsD, application specific statistics) but every monitoring system expects a consistent and uniform data model for the monitoring data it collects. By using the adapter pattern of composite containers, you can transform the heterogeneous monitoring data from different systems into a single unified representation by creating Pods that groups the application containers with adapters that know how to do the transformation. Again because these Pods share namespaces and file systems, the coordination of these two containers is simple and straightforward. https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/ https://matthewpalmer.net/kubernetes-app-developer/articles/multi-container-pod-design-patterns.html\n"
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " About me.!  Opensource Evangelist with a decade of experience Certified Kubernetes Administrator. Linux Foundation Certified Engineer. AWS Certified Solution Architect Associate. Red Hat Certified Engineer.\nHistory of K8S 2003-2004: Google introduced Borg system , which started as a small project to manage new search engine. Later on it was heavily used for managing internal distributed systems and jobs\n 2013: Google moved from Borg to Omega - a flexible and scalable scheduler for large clusters\n 2014: Google introduced kubernetes and big players (IBM, Docker, RedHat, Microsoft) joined the project\n 2015: Kubernetes 1.0 released and Google partnered with Linux Foundation to form the Cloud Native Computing Foundation (CNCF)\n 2016: Kubernetes went to mainstream and Helm package manager introduced and minikube was also released. Windows support added to k8s\n 2017: Kubernetes reached v.1.7 and were widely adopted by industry. IBM and Google introduced Istio service mesh.\n 2018: Industry understands the power of k8s and adoption rate increased\n 2019: Journey continues\u0026hellip;\n  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]